# Demo Testing & Quality Assurance (Pháº§n 9)

## ðŸŽ¯ Má»¥c tiÃªu Demo
Chá»©ng minh code quality cao vá»›i comprehensive test suite, Ä‘áº£m báº£o reliability vÃ  maintainability.

## ðŸ“‹ Chuáº©n bá»‹

### CÃ i Ä‘áº·t test dependencies (náº¿u chÆ°a cÃ³)
```powershell
pip install pytest pytest-cov pytest-xdist httpx
```

### Kiá»ƒm tra test files cÃ³ sáºµn
```powershell
ls tests/
```

**Test files:**
- `test_data.py` - Data pipeline tests
- `test_model.py` - Model architecture tests
- `test_api.py` - API integration tests
- `conftest.py` - Test fixtures vÃ  configuration

---

## ðŸŽ¬ Ká»‹ch báº£n Demo (7-10 phÃºt)

### **BÆ°á»›c 1: Giá»›i thiá»‡u Test Strategy (1 phÃºt)**

**Hiá»ƒn thá»‹ test structure:**
```powershell
tree tests /F
```

**Giáº£i thÃ­ch Test Pyramid:**
```
        /\
       /  \     E2E Tests (Ã­t)
      /----\
     /      \   Integration Tests (vá»«a)
    /--------\
   /          \ Unit Tests (nhiá»u)
  /------------\
```

**NÃ³i:** *"ChÃºng ta follow test pyramid: nhiá»u unit tests, vá»«a pháº£i integration tests. Äáº£m báº£o code quality vÃ  prevent regressions."*

---

### **BÆ°á»›c 2: Demo Unit Tests - Data Module (2 phÃºt)**

#### 2.1. Xem test code
```powershell
code tests/test_data.py
```

**Highlight test cases:**
- âœ… `test_rice_dataset` - Dataset creation
- âœ… `test_dataset_getitem` - Data loading
- âœ… `test_train_transforms` - Augmentation
- âœ… `test_class_distribution` - Data balance

#### 2.2. Run data tests
```powershell
pytest tests/test_data.py -v
```

**Expected output:**
```
tests/test_data.py::test_rice_dataset PASSED
tests/test_data.py::test_dataset_getitem PASSED
tests/test_data.py::test_train_transforms PASSED
tests/test_data.py::test_val_transforms PASSED
tests/test_data.py::test_class_distribution PASSED

====== 5 passed in 2.34s ======
```

**NÃ³i:** *"Data tests Ä‘áº£m báº£o data pipeline hoáº¡t Ä‘á»™ng Ä‘Ãºng - load data, transforms, class distribution."*

---

### **BÆ°á»›c 3: Demo Unit Tests - Model Module (2 phÃºt)**

#### 3.1. Xem test code
```powershell
code tests/test_model.py
```

**Highlight:**
```python
def test_model_forward():
    """Test model forward pass."""
    model = create_model("efficientnet_b0", num_classes=6)
    x = torch.randn(4, 3, 224, 224)
    output = model(x)
    assert output.shape == (4, 6)
```

**Parametrized tests:**
```python
@pytest.mark.parametrize("num_classes", [2, 6, 10])
def test_different_num_classes(num_classes):
    # Test vá»›i multiple configurations
```

#### 3.2. Run model tests
```powershell
pytest tests/test_model.py -v
```

**Expected output:**
```
tests/test_model.py::test_create_model PASSED
tests/test_model.py::test_model_forward PASSED
tests/test_model.py::test_model_parameters PASSED
tests/test_model.py::test_different_num_classes[2] PASSED
tests/test_model.py::test_different_num_classes[6] PASSED
tests/test_model.py::test_different_num_classes[10] PASSED

====== 6 passed in 3.21s ======
```

**NÃ³i:** *"Model tests verify architecture Ä‘Ãºng - output shapes, parameter counts, compatibility vá»›i different num_classes."*

---

### **BÆ°á»›c 4: Demo Integration Tests - API (2 phÃºt)**

#### 4.1. Xem test code
```powershell
code tests/test_api.py
```

**Highlight fixtures:**
```python
@pytest.fixture
def client():
    from api.app import app
    return TestClient(app)

@pytest.fixture
def sample_image():
    img = Image.new("RGB", (224, 224))
    # ... return image bytes
```

**Test cases:**
- âœ… `test_root_endpoint` - Root route
- âœ… `test_health_endpoint` - Health check
- âœ… `test_predict_endpoint_with_image` - Inference

#### 4.2. Run API tests
```powershell
pytest tests/test_api.py -v
```

**Expected output:**
```
tests/test_api.py::test_root_endpoint PASSED
tests/test_api.py::test_health_endpoint PASSED
tests/test_api.py::test_model_info_endpoint PASSED
tests/test_api.py::test_predict_endpoint_no_file PASSED
tests/test_api.py::test_predict_endpoint_with_image PASSED

====== 5 passed in 1.87s ======
```

**NÃ³i:** *"API tests ensure endpoints hoáº¡t Ä‘á»™ng Ä‘Ãºng - status codes, response format, error handling."*

---

### **BÆ°á»›c 5: Demo Test Coverage (1 phÃºt)**

#### 5.1. Run tests vá»›i coverage
```powershell
pytest --cov=src --cov=api --cov-report=html --cov-report=term
```

**Expected output:**
```
---------- coverage: platform win32, python 3.9 ----------
Name                    Stmts   Miss  Cover
-------------------------------------------
api\__init__.py             1      0   100%
api\app.py                 89     12    87%
src\__init__.py             2      0   100%
src\data\__init__.py       15      2    87%
src\data\dataset.py        45      5    89%
src\models\__init__.py     12      1    92%
src\models\classifier.py   67      8    88%
-------------------------------------------
TOTAL                     231     28    88%
```

**NÃ³i:** *"88% coverage - majority cá»§a code Ä‘Æ°á»£c test. Má»¥c tiÃªu > 80% coverage."*

#### 5.2. Xem HTML coverage report
```powershell
start htmlcov/index.html
```

**Chá»‰ vÃ o:**
- Green lines - covered
- Red lines - not covered
- Identify untested code paths

---

### **BÆ°á»›c 6: Demo Parallel Testing (1 phÃºt)**

#### 6.1. Run tests in parallel
```powershell
pytest -n auto -v
```

**Output:**
```
gw0 [16] / gw1 [16] / gw2 [16] / gw3 [16]
... tests running in parallel ...

====== 16 passed in 1.2s (0:00:01) ======
```

**NÃ³i:** *"Pytest-xdist cháº¡y tests parallel. Tá»« 5 seconds xuá»‘ng cÃ²n 1.2 seconds - save time trong CI/CD."*

---

### **BÆ°á»›c 7: Demo Test in Docker (1 phÃºt)**

#### 7.1. Create test Dockerfile (giáº£i thÃ­ch)
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["pytest", "-v", "--cov=src", "--cov=api"]
```

#### 7.2. Run tests in container
```powershell
docker build -t rice-disease-tests -f docker/Dockerfile.test .
docker run --rm rice-disease-tests
```

**NÃ³i:** *"Tests cháº¡y trong Docker Ä‘á»ƒ ensure consistency. Same environment cho CI/CD."*

---

### **BÆ°á»›c 8: Demo CI/CD Integration (1 phÃºt)**

#### 8.1. GitHub Actions workflow (giáº£i thÃ­ch)
```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest --cov --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v2
```

**NÃ³i:** *"Tests tá»± Ä‘á»™ng cháº¡y má»—i khi push code. Block merge náº¿u tests fail hoáº·c coverage drop."*

---

### **BÆ°á»›c 9: Demo Model Quality Tests (1 phÃºt)**

#### 9.1. Model performance tests
```powershell
code tests/test_model_quality.py
```

**Test minimum accuracy:**
```python
def test_model_accuracy():
    """Test model meets minimum accuracy threshold."""
    model = load_model("models/best_model.pth")
    val_loader = create_dataloader("validation/", batch_size=32)

    accuracy = evaluate(model, val_loader)
    assert accuracy >= 0.85, f"Accuracy {accuracy} below threshold 0.85"

def test_inference_speed():
    """Test inference speed within acceptable range."""
    model = load_model("models/best_model.pth")
    img = torch.randn(1, 3, 224, 224)

    import time
    start = time.time()
    output = model(img)
    duration = time.time() - start

    assert duration < 0.5, f"Inference took {duration}s, threshold 0.5s"
```

**NÃ³i:** *"Quality tests ensure model performance - minimum accuracy, inference speed, no degradation over time."*

---

### **BÆ°á»›c 10: Demo Stress Testing (optional)**

#### 10.1. API load testing
```powershell
# Install locust
pip install locust

# Create locustfile.py
code tests/locustfile.py
```

```python
from locust import HttpUser, task, between

class APIUser(HttpUser):
    wait_time = between(1, 2)

    @task
    def predict(self):
        files = {'file': open('test_image.jpg', 'rb')}
        self.client.post("/predict", files=files)
```

**Run load test:**
```powershell
locust -f tests/locustfile.py --host=http://localhost:8000
```

**NÃ³i:** *"Load testing Ä‘áº£m báº£o API handle concurrent requests. Monitor response time, error rate."*

---

### **BÆ°á»›c 11: Tá»•ng káº¿t (30s)**

**Test Coverage Summary:**
- âœ… **Unit Tests** - 11 tests cho data, model modules (88% coverage)
- âœ… **Integration Tests** - 5 tests cho API endpoints
- âœ… **Quality Tests** - Model accuracy, inference speed
- âœ… **CI/CD** - Automated testing trong pipeline
- âœ… **Parallel Execution** - Fast feedback (1.2s total)

**Quality Metrics:**
- ðŸ“Š **Test Coverage**: 88% (target > 80%)
- âš¡ **Test Speed**: 1.2s parallel, 5s sequential
- âœ… **Pass Rate**: 16/16 tests passing
- ðŸ”’ **No Regressions**: Tests prevent code quality degradation

**NÃ³i:** *"Comprehensive test suite Ä‘áº£m báº£o code quality. Tests cháº¡y tá»± Ä‘á»™ng, catch bugs early, enable safe refactoring."*

---

## ðŸŽ¯ Q&A ThÆ°á»ng gáº·p

### Q1: "80% coverage cÃ³ Ä‘á»§ khÃ´ng?"
**A:**
- 80% lÃ  baseline tá»‘t
- Critical paths (inference, data loading) pháº£i 100%
- Config/utils cÃ³ thá»ƒ lower coverage
- Focus on meaningful tests, khÃ´ng pháº£i chá»‰ coverage number

### Q2: "Unit test vs Integration test - khi nÃ o dÃ¹ng gÃ¬?"
**A:**
- **Unit tests** - Test 1 function/class isolated, fast, nhiá»u
- **Integration tests** - Test components work together, slower, Ã­t hÆ¡n
- **E2E tests** - Test full workflow, slowest, ráº¥t Ã­t

### Q3: "LÃ m sao Ä‘á»ƒ test model training?"
**A:**
```python
def test_training_step():
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters())
    x = torch.randn(2, 3, 224, 224)
    y = torch.tensor([0, 1])

    # Forward pass
    output = model(x)
    loss = F.cross_entropy(output, y)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Check gradients exist
    assert all(p.grad is not None for p in model.parameters())
```

### Q4: "Test trong Docker vs local cÃ³ khÃ¡c biá»‡t?"
**A:**
- Docker: Consistent environment, same as CI/CD
- Local: Faster, easier to debug
- Best practice: Local cho dev, Docker cho CI/CD

### Q5: "LÃ m sao Ä‘á»ƒ test async code trong FastAPI?"
**A:**
```python
import pytest

@pytest.mark.asyncio
async def test_async_endpoint():
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.get("/async-endpoint")
    assert response.status_code == 200
```

---

## ðŸ“Š Demo Metrics

### Test Execution Time
```powershell
pytest --durations=10
```

### Test Coverage by Module
```powershell
pytest --cov=src --cov-report=term-missing
```

### Failed Tests Details
```powershell
pytest -v --tb=short
```

---

## ðŸš€ Tips cho Demo mÆ°á»£t mÃ 

1. **Chuáº©n bá»‹ trÆ°á»›c:**
   - Run tests 1 láº§n Ä‘á»ƒ ensure all passing
   - Generate coverage report HTML
   - CÃ³ backup screenshots náº¿u tests fail

2. **Trong lÃºc demo:**
   - Start vá»›i simple unit tests
   - Show test code â†’ Run tests â†’ Show results
   - Highlight coverage gaps â†’ Explain strategy

3. **Visual aids:**
   - Coverage report HTML (green/red highlighting)
   - pytest output vá»›i colors (-v flag)
   - CI/CD pipeline screenshot (GitHub Actions)

4. **Common issues:**
   - Import errors â†’ Check PYTHONPATH
   - Model not found â†’ Skip model-dependent tests vá»›i `pytest.skip()`
   - Slow tests â†’ Use `-k` to run subset

---

## âœ… Checklist trÆ°á»›c khi Demo

- [ ] All tests passing: `pytest`
- [ ] Coverage report generated: `pytest --cov --cov-report=html`
- [ ] No warnings: `pytest -W error::DeprecationWarning`
- [ ] Tests run in Docker: `docker build -t tests .`
- [ ] CÃ³ test code má»Ÿ sáºµn trong editor
- [ ] Terminal clean (clear screen)

---

## ðŸ”— Quick Demo Commands

```powershell
# Run all tests
pytest -v

# Run specific test file
pytest tests/test_api.py -v

# Run with coverage
pytest --cov=src --cov=api --cov-report=html

# Run parallel
pytest -n auto

# Run and stop on first failure
pytest -x

# Run only failed tests from last run
pytest --lf

# Show print statements
pytest -s

# Run tests matching pattern
pytest -k "test_model"

# Generate JUnit XML report (for CI/CD)
pytest --junitxml=test-results.xml
```

---

## ðŸ“ Bonus: Create Missing Tests

### Test model quality
```python
# tests/test_model_quality.py
def test_model_accuracy_threshold():
    """Ensure model meets minimum accuracy."""
    # Load validation data
    # Run inference
    # Assert accuracy >= 85%
    pass

def test_no_overfitting():
    """Check train vs val accuracy gap."""
    # Load metrics
    # Assert (train_acc - val_acc) < 10%
    pass
```

### Test data validation
```python
# tests/test_data_validation.py
def test_no_corrupted_images():
    """Check all images can be loaded."""
    # Iterate through dataset
    # Try to load each image
    # Assert no errors
    pass

def test_class_balance():
    """Check classes are reasonably balanced."""
    # Get class distribution
    # Assert max_class / min_class < 3
    pass
```

---

**Thá»i gian demo**: 7-10 phÃºt
**Äá»™ khÃ³**: Trung bÃ¬nh-Cao
**Impact**: Ráº¥t cao - Chá»©ng minh code quality vÃ  reliability
